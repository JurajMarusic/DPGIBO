{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore all warnings|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff3(x, noise_std=0):\n",
    "    n, d = x.size()\n",
    "    center = torch.tensor([0.0], dtype=x.dtype).unsqueeze(-2)\n",
    "    return 0.5 * (x - center).square().sum(dim=-1) + noise_std * torch.randn(\n",
    "        n, device=x.device\n",
    "    )\n",
    "\n",
    "\n",
    "def ff1(x, noise_std=0):\n",
    "    n, d = x.size()\n",
    "    center = torch.tensor([2.0], dtype=x.dtype).unsqueeze(-2)\n",
    "    return -torch.exp(-(x - center).square() / 2).sum(dim=-1) + noise_std * torch.randn(\n",
    "        n, device=x.device\n",
    "    )\n",
    "\n",
    "\n",
    "def ff2(x, noise_std=1e-2):\n",
    "    n, d = x.size()\n",
    "    return torch.pow(x, 4).sum(dim=-1) + noise_std * torch.randn(n, device=x.device)\n",
    "\n",
    "\n",
    "def L_huber(theta, x, delta=5.0):\n",
    "    n, d = theta.size()\n",
    "    residual = theta - x\n",
    "    residual = torch.abs(residual)\n",
    "\n",
    "    sol = residual.sum(dim=1)\n",
    "\n",
    "    # Compute Huber loss element-wise\n",
    "    abs_residual = torch.abs(residual)\n",
    "    huber_loss = torch.where(\n",
    "        abs_residual <= delta, 0.5 * residual**2, delta * abs_residual - 0.5 * delta**2\n",
    "    )\n",
    "\n",
    "    # Sum the Huber loss over all dimensions\n",
    "    sol = huber_loss.sum(dim=1)\n",
    "\n",
    "    return sol\n",
    "\n",
    "\n",
    "def huber(theta, data, delta=1.0, noise_std=0):\n",
    "\n",
    "    n, d = theta.size()\n",
    "    residual = theta.unsqueeze(1) - data\n",
    "\n",
    "    # Compute Huber loss element-wise\n",
    "    abs_residual = torch.abs(residual)\n",
    "    huber_loss = torch.where(\n",
    "        abs_residual <= delta, 0.5 * residual**2, delta * abs_residual - 0.5 * delta**2\n",
    "    )\n",
    "\n",
    "    # Sum the Huber loss over all dimensions\n",
    "    sol = -huber_loss.sum(dim=1).sum(dim=1) / 2\n",
    "\n",
    "    # Add noise term if specified\n",
    "    if noise_std > 0:\n",
    "        sol += noise_std * torch.randn(n)\n",
    "\n",
    "    return sol\n",
    "\n",
    "\n",
    "def gaussian_log_likelihood(theta, data, noise_std=0):\n",
    "    n, d = theta.size()\n",
    "    sol = -(theta.unsqueeze(1) - data).square().sum(dim=1).sum(dim=1) / 2\n",
    "    return sol + noise_std * torch.randn(n)\n",
    "\n",
    "\n",
    "def L_gaussian_ll(theta, data, noise_std=0):\n",
    "    n, d = theta.size()\n",
    "    sol = -(theta.unsqueeze(1) - data).square().sum(dim=2).T / 2\n",
    "    return sol + noise_std * torch.randn(n)\n",
    "\n",
    "\n",
    "def gaussian_log_likelihood_stochastic(theta, data, noise_std=0):\n",
    "    n, d = theta.size()\n",
    "    N, D = data.size()\n",
    "    j = np.random.randint(N)\n",
    "    sol = (theta - data[j].T).square()\n",
    "    sol = -sol.sum(dim=-1) / 2\n",
    "    return sol + noise_std * torch.randn(n)\n",
    "\n",
    "\n",
    "def gaussian_likelihood(theta, data, noise_std=0):\n",
    "    n, d = theta.size()\n",
    "    sol = -(theta.unsqueeze(1) - data).square().sum(dim=1).sum(dim=1) / 2\n",
    "    return torch.exp(sol) + noise_std * torch.randn(n)\n",
    "\n",
    "\n",
    "def kernel_fn(x1, x2, kernel=\"gaussian\"):\n",
    "    if kernel == \"gaussian\":\n",
    "        return torch.exp(-(x1.unsqueeze(1) - x2.unsqueeze(0)).square().sum(dim=2) / 2)\n",
    "\n",
    "    elif kernel == \"poly\":\n",
    "        degree = 2  # Adjust degree as needed\n",
    "        return ((x1.unsqueeze(1) * x2.unsqueeze(0)).sum(dim=2) + 1).pow(degree)\n",
    "\n",
    "\n",
    "def kernel_grad_fn(x1, x2, kernel=\"gaussian\"):\n",
    "    if kernel == \"gaussian\":\n",
    "        return -(x1 - x2).T * kernel_fn(x1, x2)\n",
    "\n",
    "    elif kernel == \"poly\":\n",
    "        degree = 2  # Adjust degree as needed\n",
    "        kernel_values = ((x1.unsqueeze(1) * x2.unsqueeze(0)).sum(dim=2) + 1).pow(\n",
    "            degree - 1\n",
    "        )\n",
    "        return x2.T * degree * kernel_values\n",
    "\n",
    "\n",
    "def kernel_grad_grad_fn(x1, x2, kernel=\"gaussian\"):\n",
    "    if kernel == \"gaussian\":\n",
    "        k = kernel_fn(x1, x2)  # Shape (batch_size,)\n",
    "        v = x1 - x2  # Shape (batch_size, dim)\n",
    "\n",
    "        v_outer = torch.outer(v.squeeze(), v.squeeze())\n",
    "\n",
    "        identity = torch.eye(x1.size(-1), device=device)\n",
    "\n",
    "        hessian = k * (-v_outer + identity)\n",
    "\n",
    "        return hessian\n",
    "\n",
    "    elif kernel == \"poly\":\n",
    "        degree = 2\n",
    "        k = kernel_fn(x1, x2, kernel=\"poly\")  # Shape (batch_size, batch_size)\n",
    "        dot_products = (x1.unsqueeze(1) * x2.unsqueeze(0)).sum(\n",
    "            dim=2\n",
    "        ) + 1  # Shape (batch_size, batch_size)\n",
    "\n",
    "        grad = degree * dot_products.pow(degree - 1)  # First-order gradient factor\n",
    "\n",
    "        hessian = degree * (degree - 1) * dot_products.pow(degree - 2).unsqueeze(\n",
    "            -1\n",
    "        ).unsqueeze(-1) * (\n",
    "            x2.unsqueeze(0).unsqueeze(-1) @ x2.unsqueeze(0).unsqueeze(-2)\n",
    "        ) + grad.unsqueeze(\n",
    "            -1\n",
    "        ).unsqueeze(\n",
    "            -1\n",
    "        ) * torch.eye(\n",
    "            x1.size(-1)\n",
    "        ).unsqueeze(\n",
    "            0\n",
    "        ).unsqueeze(\n",
    "            0\n",
    "        )\n",
    "        return hessian.squeeze()\n",
    "\n",
    "\n",
    "def get_posterior_variance_of_gradient(x, z, kernel=\"gaussian\"):\n",
    "\n",
    "    K_xx = kernel_grad_grad_fn(x, x, kernel=kernel)\n",
    "    K_xz = kernel_grad_fn(x, z, kernel=kernel)\n",
    "    K_zz = kernel_fn(z, z, kernel=kernel)\n",
    "    # K_zz = K_zz @ K_zz\n",
    "\n",
    "    jitter = 1e-6 * 5  # This value can be tuned depending on the problem\n",
    "\n",
    "    L = torch.cholesky(K_zz + (jitter) * torch.eye(K_zz.size(-1), device=device))\n",
    "    v = torch.cholesky_solve(K_xz.T, L)\n",
    "\n",
    "    result = K_xx - torch.matmul(K_xz, v)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_posterior_mean_of_gradient(x, z, kernel=\"gaussian\"):\n",
    "\n",
    "    K_xz = kernel_grad_fn(x, z, kernel=kernel)\n",
    "    K_zz = kernel_fn(z, z, kernel=kernel)\n",
    "    y = ff(z).unsqueeze(-1)\n",
    "\n",
    "    jitter = 1e-9  # This value can be tuned depending on the problem\n",
    "\n",
    "    L = torch.cholesky(K_zz + (jitter) * torch.eye(K_zz.size(-1), device=device))\n",
    "    v = torch.cholesky_solve(y, L)\n",
    "\n",
    "    result = torch.matmul(K_xz, v)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def alpha_trace(x, z, kernel=\"gaussian\"):\n",
    "\n",
    "    result = get_posterior_variance_of_gradient(x, z, kernel=kernel)\n",
    "    # result = rotation_matrix @ result\n",
    "    trace = result.diag().sum()\n",
    "\n",
    "    return trace\n",
    "\n",
    "\n",
    "def get_points(x, batch_size, D=torch.empty(0), kernel=\"gaussian\"):\n",
    "\n",
    "    d = x.size(-1)\n",
    "    z = x + 1e-5 / math.sqrt(d) * torch.randn(\n",
    "        batch_size, d, dtype=x.dtype, device=device\n",
    "    )  # starting point for z\n",
    "    z = (\n",
    "        z.clone().detach().requires_grad_(True)\n",
    "    )  # turn on gradietns since we optimize over z\n",
    "\n",
    "    optimizer = torch.optim.Adam([z], lr=0.01)\n",
    "    # optimizer = FullBatchLBFGS([z], lr=1e-1)\n",
    "\n",
    "    num_iterations = 1000\n",
    "    for i in range(num_iterations):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # loss = alpha_trace(x, z, kernel = kernel)\n",
    "        z_D = torch.concat([D, z])\n",
    "        loss = alpha_trace(x, z_D, kernel=kernel)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_points(tensor, point, C):\n",
    "    \"\"\"\n",
    "    Find the C closest points to a given point in a tensor.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): The tensor of shape (N, D), where N is the number of points, and D is the dimensionality.\n",
    "        point (torch.Tensor): A point of shape (D,) to which the distances are calculated.\n",
    "        C (int): The number of closest points to return.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The C closest points to the given point.\n",
    "    \"\"\"\n",
    "\n",
    "    C = np.minimum(tensor.size()[0], C)\n",
    "\n",
    "    if not isinstance(tensor, torch.Tensor) or not isinstance(point, torch.Tensor):\n",
    "        raise ValueError(\n",
    "            \"Both the tensor and the point must be torch.Tensor instances.\"\n",
    "        )\n",
    "\n",
    "    if tensor.shape[1] != point.shape[0]:\n",
    "        raise ValueError(\"The dimensionality of the tensor and the point must match.\")\n",
    "\n",
    "    # Compute Euclidean distances\n",
    "    distances = torch.norm(tensor - point, dim=1)\n",
    "\n",
    "    # Find the indices of the C smallest distances\n",
    "    closest_indices = torch.topk(distances, C, largest=False).indices\n",
    "\n",
    "    # Extract the C closest points\n",
    "    closest_points = tensor[closest_indices]\n",
    "\n",
    "    return closest_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated hyperparameters: tensor([[ 2.7964, -1.4322, -2.1326, -2.4699, -2.8867, -2.7668, -1.9734, -2.4639,\n",
      "         -1.5920, -1.1162,  1.7408,  0.5653,  0.4658,  2.4793,  1.2414,  2.0408,\n",
      "          2.4105, -1.6197,  0.4751, -3.7928, -1.3582,  1.3338]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import Bounds\n",
    "from scipy.optimize import minimize\n",
    "import torch\n",
    "\n",
    "# Load the CT slice dataset\n",
    "data = pd.read_csv(\"slice_localization_data.csv\")\n",
    "\n",
    "# Separate inputs (X) and output (y)\n",
    "X = data.iloc[:, 1:20].values  # All columns except the last are features\n",
    "y = data.iloc[:, -1].values  # The last column is the response\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=1000, test_size=9000, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize the data\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "y_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "y_test = scaler_y.transform(y_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Define bounds for hyperparameters in log space\n",
    "n_features = X_train.shape[1]  # Number of features\n",
    "bounds = Bounds(\n",
    "    [-3] * n_features + [np.log(0.01), np.log(0.1), np.log(0.01)],\n",
    "    [3] * n_features + [np.log(1.0), np.log(3.0), np.log(5.0)],\n",
    ")\n",
    "\n",
    "\n",
    "# Generate random hyperparameters with specific ranges\n",
    "def generate_hyperparams(d):\n",
    "    hyperparams = torch.empty(1, d).uniform_(-3, 3)  # First d-3 entries uniform(-2, 4)\n",
    "    hyperparams[0, -3] = torch.empty(1).uniform_(\n",
    "        np.log(0.01), np.log(1.0)\n",
    "    )  # Last three entries\n",
    "    hyperparams[0, -2] = torch.empty(1).uniform_(np.log(0.1), np.log(3.0))\n",
    "    hyperparams[0, -1] = torch.empty(1).uniform_(np.log(0.01), np.log(5.0))\n",
    "    return hyperparams\n",
    "\n",
    "\n",
    "# Objective function: Fit SVR and return validation loss matrix\n",
    "def objective_batch(batch_log_hyperparams):\n",
    "    batch_log_hyperparams = np.array(batch_log_hyperparams)\n",
    "    n = batch_log_hyperparams.shape[0]\n",
    "    validation_losses = torch.zeros((X_test.shape[0], n), dtype=torch.float64)\n",
    "\n",
    "    for i, log_hyperparams in enumerate(batch_log_hyperparams):\n",
    "        log_length_scales = log_hyperparams[:n_features]\n",
    "        log_epsilon = log_hyperparams[-3]\n",
    "        log_gamma = log_hyperparams[-2]\n",
    "        log_C = log_hyperparams[-1]\n",
    "        # Hyperparameters\n",
    "        length_scales = np.exp(log_length_scales)\n",
    "        epsilon = np.exp(log_epsilon)  # Fixed epsilon\n",
    "        gamma = np.exp(log_gamma)  # Fixed gamma\n",
    "        C = np.exp(log_C)  # Fixed C\n",
    "\n",
    "        # Scale features based on length scales\n",
    "        X_train_scaled = X_train / length_scales\n",
    "        X_test_scaled = X_test / length_scales\n",
    "\n",
    "        # Fit SVR\n",
    "        svr = SVR(kernel=\"rbf\", epsilon=epsilon, gamma=gamma, C=C)\n",
    "        svr.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Predict and compute residuals\n",
    "        y_pred = svr.predict(X_test_scaled)\n",
    "        residuals = y_test - y_pred\n",
    "\n",
    "        # Compute per-user validation loss\n",
    "        validation_losses[:, i] = torch.tensor(\n",
    "            0.5 * np.power(residuals, 2), dtype=torch.float64\n",
    "        )\n",
    "\n",
    "    return validation_losses\n",
    "\n",
    "\n",
    "# Example usage\n",
    "d = n_features + 3  # Total dimensions including additional hyperparameters\n",
    "initial_hyperparams = generate_hyperparams(d)\n",
    "print(\"Generated hyperparameters:\", initial_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff(theta1, noise_std=0.0):\n",
    "\n",
    "    n, d = theta1.size()\n",
    "    theta1 = torch.tensor(theta1, device=device)\n",
    "    return objective_batch(theta1)\n",
    "\n",
    "\n",
    "def run2(mu=1, bs=d + 1, epsilon=1, private=True, use_adaptive=False, lr=1):\n",
    "\n",
    "    D = torch.empty(0, device=device)\n",
    "    sigma = 2 * B * np.sqrt(T) / (N * mu)\n",
    "\n",
    "    theta = (\n",
    "        torch.rand(1, len(bounds.lb), device=device) * (bounds.ub - bounds.lb)\n",
    "        + bounds.lb\n",
    "    )\n",
    "    theta = theta_0\n",
    "    # theta = torch.empty(1, d, device=device).uniform_(-4., 4.)\n",
    "\n",
    "    NN_loss = [ff(theta).mean().item()]\n",
    "    NN_trace = []\n",
    "    samples_needed = []\n",
    "    theta_path = [theta[0]]\n",
    "\n",
    "    # Initialize the historical sum of squared gradients (squared accumulator)\n",
    "    historical_grad = torch.zeros_like(theta, device=device)\n",
    "\n",
    "    for iteration in tqdm(range(T)):\n",
    "        if iteration > 0:\n",
    "            D_temp = find_closest_points(D, theta[0], C).detach()\n",
    "        else:\n",
    "            D_temp = torch.empty(0)\n",
    "\n",
    "        if use_adaptive:\n",
    "            sample_counter = 0\n",
    "\n",
    "            if iteration == 0:\n",
    "                z = get_points(theta, batch_size=1, D=D_temp, kernel=kernel)\n",
    "                D = torch.concat([D, z])\n",
    "                D_temp = torch.concat([D_temp, z])\n",
    "                sample_counter += 1\n",
    "\n",
    "            while (\n",
    "                get_posterior_variance_of_gradient(theta, D_temp, kernel=kernel).trace()\n",
    "                > epsilon\n",
    "            ):\n",
    "                z = get_points(theta, batch_size=1, D=D_temp, kernel=kernel)\n",
    "                D = torch.concat([D, z])\n",
    "                D_temp = torch.concat([D_temp, z])\n",
    "                sample_counter += 1\n",
    "\n",
    "            samples_needed.append(sample_counter)\n",
    "\n",
    "        else:\n",
    "            D_temp = torch.tensor(D_temp, device=device)\n",
    "            z = get_points(theta, batch_size=bs, D=D_temp, kernel=kernel)\n",
    "            # z = torch.abs(z)\n",
    "\n",
    "            z = torch.tensor(z, device=device)\n",
    "            D = torch.concat([D, z])\n",
    "            D_temp = torch.concat([D_temp, z])\n",
    "\n",
    "        gradients = get_posterior_mean_of_gradient(\n",
    "            theta, D_temp, kernel=kernel\n",
    "        ).detach()\n",
    "\n",
    "        clipped_gradients = gradients * torch.minimum(\n",
    "            torch.tensor(1),\n",
    "            torch.tensor(B) / torch.norm(gradients, dim=1, keepdim=True),\n",
    "        )\n",
    "\n",
    "        g = clipped_gradients.sum(dim=0) / N\n",
    "        # print(g.norm())\n",
    "\n",
    "        cov = get_posterior_variance_of_gradient(theta, D_temp, kernel=kernel)\n",
    "\n",
    "        stepSize = lr\n",
    "\n",
    "        # # Update step using AdaGrad\n",
    "        gradient = g.T + sigma * torch.randn(d, device=device)  # Compute the gradient\n",
    "        historical_grad += gradient**2  # Accumulate squared gradients\n",
    "        adjusted_step = stepSize / (\n",
    "            torch.sqrt(historical_grad) + 1e-8\n",
    "        )  # Adaptive step size\n",
    "        theta = theta - adjusted_step * gradient  # Parameter update\n",
    "        theta = torch.clamp(theta, min=-5)  # Clamp values\n",
    "\n",
    "        NN_loss.append(ff(theta).mean().item())\n",
    "        NN_trace.append(cov.trace().item())\n",
    "        theta_path.append(theta[0])\n",
    "\n",
    "    return NN_loss, theta_path, NN_trace, samples_needed, g.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation parameters\n",
    "d = n_features + 3\n",
    "num_iterations = 1\n",
    "path_length = 30\n",
    "T = path_length\n",
    "bs = d + 1\n",
    "mu = 0.2\n",
    "B = 0.5\n",
    "N = 9000\n",
    "C = 0\n",
    "kernel = \"gaussian\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_evaluate(T):\n",
    "    \"\"\"\n",
    "    Randomly samples hyperparameters, evaluates the validation loss T times,\n",
    "    and plots the best guess (lowest validation loss so far) over iterations.\n",
    "\n",
    "    Parameters:\n",
    "    - T (int): Number of iterations (hyperparameter samples).\n",
    "    - X_train (torch.Tensor): Training features of shape (m_train, d).\n",
    "    - Y_train (torch.Tensor): Training targets of shape (m_train, 1).\n",
    "    - X_val (torch.Tensor): Validation features of shape (m_val, d).\n",
    "    - Y_val (torch.Tensor): Validation targets of shape (m_val, 1).\n",
    "    \"\"\"\n",
    "    best_loss_so_far = []\n",
    "    best_loss = float(\"inf\")  # Initialize the best loss to infinity\n",
    "\n",
    "    for t in tqdm(range(T)):\n",
    "        # Randomly sample hyperparameters in R\n",
    "        # hyperparams = torch.randn(1, 10)  # One set of hyperparameters\n",
    "        hyperparams = (\n",
    "            torch.rand(1, len(bounds.lb)) * (bounds.ub - bounds.lb) + bounds.lb\n",
    "        )\n",
    "\n",
    "        # Evaluate validation losses\n",
    "        validation_losses = objective_batch(hyperparams)\n",
    "\n",
    "        # Compute average validation loss across users\n",
    "        avg_loss = validation_losses.mean().item()\n",
    "\n",
    "        # Update the best loss so far\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "        best_loss_so_far.append(best_loss)\n",
    "\n",
    "    return best_loss_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003042903097250923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [03:27<00:00,  6.90s/it]\n",
      "100%|██████████| 30/30 [03:48<00:00,  7.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP-GIBO 0.1440074933031785\n",
      "DP-GIBO2 0.13612444096745457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(2 * B * np.sqrt(T) / (N * mu))\n",
    "import os\n",
    "\n",
    "# Arrays to store losses\n",
    "losses_dp_gibo = np.zeros((num_iterations, path_length + 1))\n",
    "losses_dp_gibo2 = np.zeros((num_iterations, path_length + 1))\n",
    "losses_random = np.zeros((num_iterations, path_length * bs))\n",
    "\n",
    "# os.makedirs(\"d100\", exist_ok=True)\n",
    "\n",
    "theta_0 = (\n",
    "    torch.rand(1, len(bounds.lb), device=device) * (bounds.ub - bounds.lb) + bounds.lb\n",
    ")\n",
    "\n",
    "# Run the simulations\n",
    "for i in range(num_iterations):\n",
    "    loss, path, trace_path, samples_needed, norm = run2(\n",
    "        bs=bs, epsilon=2, mu=mu, lr=1.0\n",
    "    )  # Simulated DP-GIBO loss\n",
    "    losses_dp_gibo[i, :] = loss\n",
    "\n",
    "    loss2, path2, trace_path2, samples_needed2, norm2 = run2(\n",
    "        bs=bs, epsilon=2, mu=10000, lr=1.0\n",
    "    )  # Simulated DP-GIBO loss\n",
    "    losses_dp_gibo2[i, :] = loss2\n",
    "\n",
    "    # loss_random = sample_and_evaluate(\n",
    "    #     T=path_length * bs,\n",
    "    # )\n",
    "    # losses_random[i, :] = torch.tensor(loss_random)\n",
    "\n",
    "    print(\"DP-GIBO\", loss[-1])\n",
    "    print(\"DP-GIBO2\", loss2[-1])\n",
    "    # print(\"RANDOM\", loss_random[-1])\n",
    "\n",
    "# np.save(f\"d100/losses_dp_gibo.npy\", losses_dp_gibo)\n",
    "# np.save(f\"d100/losses_random.npy\", losses_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tensors to files\n",
    "torch.save(losses_dp_gibo, \"losses_dp_gibo.pt\")\n",
    "torch.save(losses_random, \"losses_random.pt\")\n",
    "torch.save(losses_dp_gibo_inf, \"losses_dp_gibo_inf.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load tensors from files\n",
    "losses_dp_gibo = torch.load(\"losses_dp_gibo.pt\")\n",
    "losses_random = torch.load(\"losses_random.pt\")\n",
    "losses_dp_gibo_inf = torch.load(\"losses_dp_gibo_inf.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
